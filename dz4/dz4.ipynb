{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9331d76",
   "metadata": {},
   "source": [
    "# Горбунов Иван ИУ6-54б"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74852309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Задача № 1:  3\n",
      "Задача № 2:  1\n"
     ]
    }
   ],
   "source": [
    "surname = \"Горбунов\"\n",
    "\n",
    "alp = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "w = [1, 42, 21, 21, 34,  6, 44, 26, 18, 44, 38, 26, 14, 43,  4, 49, 45,\n",
    "        7, 42, 29,  4,  9, 36, 34, 31, 29,  5, 30,  4, 19, 28, 25, 33]\n",
    "\n",
    "d = dict(zip(alp, w))\n",
    "variant =  sum([d[el] for el in surname.lower()]) % 40 + 1\n",
    "\n",
    "print(\"Задача № 1: \", variant % 3 + 1)\n",
    "print(\"Задача № 2: \", variant % 2 + 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c07d9",
   "metadata": {},
   "source": [
    "## Настройка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "556d1209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 14:20:01 WARN Utils: Your hostname, MacBook.local resolves to a loopback address: 127.0.0.1; using 192.168.31.202 instead (on interface en0)\n",
      "26/01/11 14:20:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/11 14:20:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark MLlib готов к работе!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Настройка окружения для macOS\n",
    "os.environ[\"OBJC_DISABLE_INITIALIZE_FORK_SAFETY\"] = \"YES\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python3\"\n",
    "# Путь к Java\n",
    "java_path = \"/Library/Java/JavaVirtualMachines/temurin-17.jdk/Contents/Home\" \n",
    "if not os.path.exists(java_path):\n",
    "    java_path = \"/Library/Java/JavaVirtualMachines/openjdk-17.jdk/Contents/Home\"\n",
    "os.environ[\"JAVA_HOME\"] = java_path\n",
    "\n",
    "# Запуск Spark Session\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HW4_MachineLearning\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark MLlib готов к работе!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17bae1",
   "metadata": {},
   "source": [
    "## Задание 1 - Классификация (Вариант 3: Spambase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec1b5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные загружены: 4601 строк\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 14:20:07 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результат задания 1 (Классификация)\n",
      "Dataset: Spambase\n",
      "Точность модели (Accuracy): 93.15%\n",
      "Пример предсказаний:\n",
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|    1|       1.0|[0.11453432509771...|\n",
      "|    1|       1.0|[0.04224703991056...|\n",
      "|    1|       1.0|[0.04826249244919...|\n",
      "|    1|       1.0|[0.04826249244919...|\n",
      "|    1|       1.0|[0.04649753518424...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import urllib.request\n",
    "\n",
    "# Скачиваем датасет\n",
    "url_spam = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "file_spam = \"spambase.data\"\n",
    "\n",
    "if not os.path.exists(file_spam):\n",
    "    print(\"Скачивание Spambase...\")\n",
    "    urllib.request.urlretrieve(url_spam, file_spam)\n",
    "\n",
    "# Загружаем данные\n",
    "data_spam = spark.read.csv(file_spam, inferSchema=True, header=False)\n",
    "\n",
    "print(f\"Данные загружены: {data_spam.count()} строк\")\n",
    "\n",
    "input_cols = [f\"_c{i}\" for i in range(57)]\n",
    "data_spam = data_spam.withColumnRenamed(\"_c57\", \"label\")\n",
    "\n",
    "# Подготовка признаков\n",
    "# Spark ML принимает все признаки в виде одного вектора\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "final_data = assembler.transform(data_spam).select(\"features\", \"label\")\n",
    "\n",
    "# Разделение на обучение и тест (70% / 30%)\n",
    "train_data, test_data = final_data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Обучение модели - RandomForest\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n",
    "model = rf.fit(train_data)\n",
    "\n",
    "# Предсказание и Оценка\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Оцениваем точность (Accuracy)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"\\nРезультат задания 1 (Классификация)\")\n",
    "print(f\"Dataset: Spambase\")\n",
    "print(f\"Точность модели (Accuracy): {accuracy:.2%}\")\n",
    "print(\"Пример предсказаний:\")\n",
    "predictions.select(\"label\", \"prediction\", \"probability\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478e082b",
   "metadata": {},
   "source": [
    "## Задание 2 - Регрессия (Вариант 1: Bike Sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ecd560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 14:20:11 WARN Instrumentation: [4b5e320e] regParam is zero, which might cause numerical instability and overfitting.\n",
      "26/01/11 14:20:11 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "26/01/11 14:20:11 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "26/01/11 14:20:11 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результат задания 2 (Регрессия)\n",
      "Ошибка (RMSE): 143.40\n",
      "Пример предсказаний:\n",
      "+---+-------------------+\n",
      "|cnt|         prediction|\n",
      "+---+-------------------+\n",
      "| 39|-26.994394850802752|\n",
      "| 89|  100.1366353796962|\n",
      "|128|  47.45281320676513|\n",
      "|173| 60.734487688388626|\n",
      "| 88| 1.2846049058215208|\n",
      "+---+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import zipfile\n",
    "\n",
    "# Скачиваем датасет\n",
    "url_bike = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\"\n",
    "zip_bike = \"Bike-Sharing-Dataset.zip\"\n",
    "csv_bike = \"hour.csv\"\n",
    "\n",
    "if not os.path.exists(csv_bike):\n",
    "    print(\"Скачиваю Bike Sharing Data...\")\n",
    "    urllib.request.urlretrieve(url_bike, zip_bike)\n",
    "    with zipfile.ZipFile(zip_bike, 'r') as z:\n",
    "        z.extract(csv_bike)\n",
    "\n",
    "# Загружаем данные\n",
    "df_bike = spark.read.csv(csv_bike, header=True, inferSchema=True)\n",
    "\n",
    "# Подготовка признаков\n",
    "feature_cols = ['season', 'yr', 'mnth', 'hr', 'holiday', 'workingday', 'weathersit', 'temp', 'hum', 'windspeed']\n",
    "\n",
    "assembler_reg = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "final_data_reg = assembler_reg.transform(df_bike).select(\"features\", \"cnt\")\n",
    "\n",
    "#Разделение на обучение и тест\n",
    "train_reg, test_reg = final_data_reg.randomSplit([0.7, 0.3], seed=123)\n",
    "\n",
    "# Обучение модели (Линейная регрессия)\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"cnt\")\n",
    "lr_model = lr.fit(train_reg)\n",
    "\n",
    "# Оценка\n",
    "reg_predictions = lr_model.transform(test_reg)\n",
    "reg_evaluator = RegressionEvaluator(labelCol=\"cnt\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = reg_evaluator.evaluate(reg_predictions)\n",
    "\n",
    "print(\"\\nРезультат задания 2 (Регрессия)\")\n",
    "print(f\"Ошибка (RMSE): {rmse:.2f}\")\n",
    "print(\"Пример предсказаний:\")\n",
    "reg_predictions.select(\"cnt\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc8d2b7",
   "metadata": {},
   "source": [
    "## Задание 3 - Кластеризация (Iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "286adf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результат задания 3 Кластеризация\n",
      "Silhouette Score: 0.7355\n",
      "Центры кластеров:\n",
      "[5.9016129  2.7483871  4.39354839 1.43387097]\n",
      "[5.006 3.418 1.464 0.244]\n",
      "[6.85       3.07368421 5.74210526 2.07105263]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Скачиваем датасет\n",
    "url_iris = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "file_iris = \"iris.data\"\n",
    "\n",
    "if not os.path.exists(file_iris):\n",
    "    print(\"Скачиваю Iris...\")\n",
    "    urllib.request.urlretrieve(url_iris, file_iris)\n",
    "\n",
    "# Загружаем\n",
    "df_iris = spark.read.csv(file_iris, inferSchema=True)\n",
    "\n",
    "iris_features = [\"_c0\", \"_c1\", \"_c2\", \"_c3\"]\n",
    "\n",
    "assembler_cl = VectorAssembler(inputCols=iris_features, outputCol=\"features\")\n",
    "dataset_cl = assembler_cl.transform(df_iris).select(\"features\")\n",
    "\n",
    "# Обучение K-Means\n",
    "kmeans = KMeans().setK(3).setSeed(1)\n",
    "model_cl = kmeans.fit(dataset_cl)\n",
    "\n",
    "#Предсказание кластеров\n",
    "predictions_cl = model_cl.transform(dataset_cl)\n",
    "\n",
    "# оценка качества (Silhouette score)\n",
    "# Показывает, насколько хорошо разделены кластеры (от -1 до 1)\n",
    "evaluator_cl = ClusteringEvaluator()\n",
    "silhouette = evaluator_cl.evaluate(predictions_cl)\n",
    "\n",
    "print(\"\\nРезультат задания 3 Кластеризация\")\n",
    "print(f\"Silhouette Score: {silhouette:.4f}\")\n",
    "print(\"Центры кластеров:\")\n",
    "centers = model_cl.clusterCenters()\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
